<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>

    html {
    font-size: 100%; /* typically 16px */
    }

    body {
    margin: 0;
    font-family: sans-serif;
    background: #f9f9f9;
    color: #222;
    }

    /* Top header -------------------------------------------------- */

    header {
    display: flex;
    justify-content: center;
    align-items: center;
    padding: 2rem 1rem;
    background: #fff;
    border-bottom: 5px solid #bc08e9;
    }

    #har_image {
    width: 12rem;                /* ~192px at 16px base */
    height: 12rem;
    border-radius: 8px;
    margin-bottom: 1rem;
    }

    #title {
    font-size: 1.8rem;             /* ~32px */
    font-weight: bold;
    margin-top: 1.5rem;
    text-align: center;
    }

    /* Content sections -------------------------------------------- */

    .content-section {
    display: block;              /* replaces your header2 */
    padding: 2rem 1.5rem;
    background: #fff;
    border-bottom: 5px solid #bc08e9;
    }

    /* Limit line width so it doesn't feel huge on wide screens */
    .content-inner {
    max-width: 60rem;            /* ~960px */
    margin: 0 auto;
    }

    /* Headings and lists ------------------------------------------ */

    .subheading {
    font-size: 1.5rem;           /* ~24px */
    font-weight: bold;
    margin-bottom: 0.75rem;
    }

    /* List items (publications / patents) */
    ul.publications {
    padding-left: 1.5rem;
    margin-top: 0;
    }

    ul.publications li {
    margin-bottom: 0.75rem;
    font-size: 1rem;             /* ~16px, normal body text */
    line-height: 1.6;
    }

  </style>
</head>


<body>

<header>
    <div style="text-align: center; width: 100%;">
        <img id="har_image" src="images/human_activity_recognition.png" alt="HAR" />
        <div id="title">
            ~ Cagri Tanriover's Publications & Patents in Human Activity Recognition (HAR) ~<br/>
        </div>
    </div>
</header>

<div class="content-section">
    <div class="content-inner">
        <span class="subheading">
            Peer Reviewed Publications<br/>
        </span></br>

        <ul class="publications">

            <li>
                Aslan, S., Alyuz, N., <b><u>Tanriover, C.,</u></b> Mete, S. E., Okur, E., D'Mello, S. K., & Arslan Esme, A. (2019, May).  
                <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300534" target="_blank">
                "Investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms,"
                </a>
                in Proceedings of the 2019 CHI conference on human factors in computing systems (pp. 1-12).
            </li>

            <li>
                Aslan, S., Alyuz, N., <b><u>Tanriover, C.,</u></b> Mete, S. E., Okur, E., D'Mello, S. K., & Arslan Esme, A. (2019, Jan).  
                <a href="https://arxiv.org/abs/1901.06291" target="_blank">
                "Detecting Behavioral Engagement of Students in the Wild Based on Contextual and Visual Data,"
                </a>
                arXiv preprint arXiv:1901.06291.
            </li>

            <li>
                Sherry, J., Beckwith, R., Esme, A. A., & <b><u>Tanriover, C.</u></b> (2018, March).  
                <a href="https://socialrobotsinthewild.org/wp-content/uploads/2018/02/HRI-SRW_2018_paper_3.pdf" target="_blank">
                "Getting things done in an autonomous vehicle,"
                </a>
                in Social Robots in the Wild Workshop, 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2018).
            </li>

            <li>
                Alyuz, N., Okur, E., Genc, U., Aslan, S., <b><u>Tanriover, C.,</u></b> & Esme, A. A. (2017, November).  
                <a href="https://dl.acm.org/doi/abs/10.1145/3139513.3139521" target="_blank">
                "An unobtrusive and multimodal approach for behavioral engagement detection of students,"
                </a>
                in Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education (pp. 26-32).
            </li>

            <li>
                Okur, E., Alyuz, N., Aslan, S., Genc, U., <b><u>Tanriover, C.</u></b>, & Arslan Esme, A. (2017, June).  
                <a href="https://books.google.com/books?hl=en&lr=&id=SWgpDwAAQBAJ&oi=fnd&pg=PA250&dq=cagri+tanriover&ots=vf-x0zOmgC&sig=HLbMv4-ATgcDKtH5lrQnCJGonEQ#v=onepage&q=cagri%20tanriover&f=false" target="_blank">
                "Behavioral Engagement Detection,"
                </a>
                in Artificial Intelligence in Education: 18th International Conference, AIED 2017, Wuhan, China, June 28-July 1, 2017, Proceedings (Vol. 10331, p. 250). Springer.
            </li>

            <li>
                Okur, E., Alyuz, N., Aslan, S., Genc, U., <b><u>Tanriover, C.</u></b>, & Arslan Esme, A. (2017, June).  
                <a href="https://link.springer.com/chapter/10.1007/978-3-319-61425-0_21" target="_blank">
                "Behavioral engagement detection of students in the wild,"
                </a>
                in International Conference on Artificial Intelligence in Education (pp. 250-261). Cham: Springer International Publishing.
            </li>

        </ul>
    </div>
</div>


<div class="content-section">
  <div class="content-inner">
        <span class="subheading">
            Patents<br/>
        </span></br>

        <ul class="publications">
            <li>
                Civitci, N. A., <b><u>Tanriover, C. C.,</u></b> Aslan, S., Kavil, E. O., Esme, A. A., & Genc, E. (2024)
                <a href="https://patents.google.com/patent/US11908345B2/en" target="_blank">
                “Engagement level determination and dissemination,”
                </a>
                U.S. Patent No. 11,908,345. Washington, DC: U.S. Patent and Trademark Office.
            </li>

            <li>
                <b><u>Tanriover, C.,</u></b> Beckwith, R., Esme, A. A., & Sherry, J. (2022)
                <a href="https://patents.google.com/patent/US11256104B2/en" target="_blank">
                “Intelligent vehicle point of focus communication,”
                </a>
                U.S. Patent No. 11,256,104. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

            <li>
                <b><u>Tanriover, C.,</u></b> C., Civitci, N. A., Esme, A. A., Maruri, H. A. C., & Meyer, P. L. (2021)
                <a href="https://patents.google.com/patent/US11013430B2/en" target="_blank">
                “Methods and apparatus for identifying food chewed and/or beverage drank,”
                </a>
                U.S. Patent No. 11,013,430. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

            <li>
                FOX, M. S., Pohl, D., <b><u>Tanriover, C.,</u></b> (2021)
                <a href="https://patents.google.com/patent/US20210110315A1/en" target="_blank">
                “Compatibility of ride hailing passengers,”
                </a>
                U.S. Patent Application No. 17/131,631.
            </li>  

            <li>
                <b><u>Tanriover, C.,</u></b> C., Civitci, N. A., Esme, A. A., Maruri, H. A. C., & Meyer, P. L. (2021)
                <a href="https://patents.google.com/patent/US11006875B2/en" target="_blank">
                “Technologies for emotion prediction based on breathing patterns,”
                </a>
                U.S. Patent No. 11,006,875. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

            <li>
                <b><u>Tanriover, C. C.,</u></b> (2020)
                <a href="https://patents.google.com/patent/US10852728B2/en" target="_blank">
                “Remote driver state consideration for autonomous vehicles,”
                </a>
                U.S. Patent No. 10,852,728. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

            <li>
                <b><u>Tanriover, C. C.,</u></b> Maruri, H. C., Meyer, P. L., & Esme, A. A. (2019) 
                <a href="https://patents.google.com/patent/US20190038179A1/en" target="_blank">
                “Methods and apparatus for identifying breathing patterns,”
                </a>
                U.S. Patent Application No. 15/669,137.
            </li>  

            <li>
                <b><u>Tanriover, C.,</u></b> Aslan, S., Civitci, N. A., Oktay, E., Okur, E., & Esme, A. A. (2019) 
                <a href="https://patents.google.com/patent/US10299716B2/en" target="_blank">
                “Side face image-based mental state determination,”
                </a>
                U.S. Patent No. 10,299,716. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

            <li>
                <b><u>Tanriover, C. C.,</u></b> Scanaill C.N. 2019 
                <a href="https://patents.google.com/patent/US10470674B2/en" target="_blank">
                “Technologies for a fabric acoustic sensor,”
                </a>
                U.S. Patent No. US10470674B2. Washington, DC: U.S. Patent and Trademark Office.
            </li>

            <li>
                <b><u>Tanriover, C. C.,</u></b> Luebbers, E. L. E., & Sargut, S. (2018) 
                <a href="https://patents.google.com/patent/US10112505B2/en" target="_blank">
                “Occupant profiling system,”
                </a>
                U.S. Patent No. 10,112,505. Washington, DC: U.S. Patent and Trademark Office.
            </li>  

        </ul>
    </div>
</div>

